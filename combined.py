# Combined Python File
# This file is autogenerated by combine_python_files function

# ----- Begin _globals.py -----
OWNER = 'pytest-dev'
REPO_NAME = 'pytest'

# ----- End _globals.py -----


# ----- Begin _logging.py -----
import logging
from logging.handlers import RotatingFileHandler

def setup_logging(log_file='dataset_creation.log'):
    """
    Sets up logging for the script.

    Parameters:
    - log_file (str): Path to the log file.

    Returns:
    - logger: Configured logger instance.
    """
    logger = logging.getLogger('IssueDatasetCreator')
    logger.setLevel(logging.DEBUG)  # Capture all levels of logs

    # Prevent adding multiple handlers if the logger already has them
    if not logger.handlers:
        # Create handlers
        c_handler = logging.StreamHandler()
        f_handler = RotatingFileHandler(log_file, maxBytes=5*1024*1024, backupCount=2)
        c_handler.setLevel(logging.INFO)
        f_handler.setLevel(logging.DEBUG)

        # Create formatters and add to handlers
        c_format = logging.Formatter('%(levelname)s: %(message)s')
        f_format = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        c_handler.setFormatter(c_format)
        f_handler.setFormatter(f_format)

        # Add handlers to the logger
        logger.addHandler(c_handler)
        logger.addHandler(f_handler)

    return logger


# ----- End _logging.py -----

# ----- Begin dataset.py -----
import json
import os

from embeddings import generate_code_embedding, generate_issue_description_embedding
from gitcodes import get_codebase_before_commit, get_commit_files, get_issue_commits, get_issue_description


def create_issue_dataset(repo, issue_numbers, tokenizer, model, sentencebert_model, owner, repo_name, logger, output_file='data/issue_dataset.jsonl'):
    """
    Create a dataset with specified columns for given issues and save it in JSON Lines format.

    Parameters:
    - repo: Git repository object.
    - issue_numbers (iterable): Iterable of issue numbers to process.
    - tokenizer: Tokenizer for CodeBERT.
    - model: Pre-trained CodeBERT model.
    - sentencebert_model: Pre-trained Sentence-BERT model.
    - owner (str): Repository owner.
    - repo_name (str): Repository name.
    - logger: Logger instance for logging.
    - output_file (str): Path to the output JSONL file.

    Returns:
    - None
    """
    logger.info(f"Creating dataset for issues: {list(issue_numbers)}")

    # Ensure output directory exists
    output_dir = os.path.dirname(output_file)
    os.makedirs(output_dir, exist_ok=True)

    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            for issue_number in issue_numbers:
                logger.info(f"Processing issue #{issue_number}")
                issue_commits = get_issue_commits(repo, issue_number, logger)
                if not issue_commits:
                    logger.warning(f"No commits found for issue #{issue_number}, skipping...")
                    continue

                issue_description = get_issue_description(issue_number, owner, repo_name, logger)

                # Generate embedding for issue description
                issue_description_embedding = generate_issue_description_embedding(issue_description, sentencebert_model, logger)

                # Sort commits by commit date to find the first commit
                sorted_commits = sorted(issue_commits, key=lambda c: c.committed_date)
                first_commit = sorted_commits[0]
                first_commit_hash = first_commit.hexsha
                logger.info(f"First commit for issue #{issue_number}: {first_commit_hash}")

                # Get codebase before the first commit
                codebase_before = get_codebase_before_commit(repo, first_commit, logger)

                # Generate embedding for codebase
                codebase_embedding = generate_code_embedding(codebase_before, tokenizer, model, logger)

                # Calculate NOC, NOCF, NOI, NOD
                noc = len(issue_commits)
                nocf = 0
                nod = 0
                noi = 0

                for commit in issue_commits:
                    files = get_commit_files(repo, commit.hexsha, logger)
                    nocf += len(files)
                    for file, stats in files.items():
                        nod += stats.get('deletions', 0)
                        noi += stats.get('insertions', 0)

                # Prepare the data record
                record = {
                    'issue_id': issue_number,
                    'issue_description': issue_description,
                    'issue_description_embedding': issue_description_embedding if issue_description_embedding else [],
                    'first_commit': first_commit_hash,
                    'codebase_embedding_before_first_commit': codebase_embedding if codebase_embedding else [],
                    'NOC': noc,
                    'NOCF': nocf,
                    'NOI': noi,
                    'NOD': nod
                }

                # Write the JSON object as a single line
                f.write(json.dumps(record) + '\n')

                logger.debug(f"Written data for issue #{issue_number} to {output_file}")

        logger.info(f"Dataset successfully saved to '{output_file}'")
    except Exception as e:
        logger.error(f"Error during dataset creation: {e}")


# ----- End dataset.py -----

# ----- Begin embeddings.py -----
from transformers import AutoTokenizer, AutoModel # type: ignore
from sentence_transformers import SentenceTransformer # type: ignore
import torch # type: ignore

def load_codebert_model(model_name='microsoft/codebert-base'):
    """
    Load the CodeBERT tokenizer and model.

    Parameters:
    - model_name (str): Hugging Face model identifier.

    Returns:
    - tokenizer: Tokenizer for CodeBERT.
    - model: Pre-trained CodeBERT model.
    """
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name)
    return tokenizer, model

def generate_code_embedding(codebase, tokenizer, model, logger):
    """
    Generate an embedding for the given codebase using CodeBERT.

    Parameters:
    - codebase (str): The entire codebase as a single string.
    - tokenizer: Tokenizer for CodeBERT.
    - model: Pre-trained CodeBERT model.
    - logger: Logger instance for logging.

    Returns:
    - List of floats representing the embedding vector.
    """
    if not codebase.strip():
        logger.warning("Empty codebase provided for embedding.")
        return None

    try:
        # Tokenize the input code
        inputs = tokenizer(codebase, return_tensors='pt', truncation=True, max_length=512)

        # Get model outputs
        with torch.no_grad():
            outputs = model(**inputs)

        # Perform mean pooling on the token embeddings
        embeddings = outputs.last_hidden_state.mean(dim=1).squeeze()

        # Convert to CPU and detach from the computation graph
        embeddings = embeddings.cpu().numpy()

        return embeddings.tolist()
    except Exception as e:
        logger.error(f"Error during embedding generation: {e}")
        return None

def load_sentencebert_model(model_name='sentence-transformers/all-MiniLM-L6-v2', logger=None):
    """
    Load the Sentence-BERT model for embedding issue descriptions.

    Parameters:
    - model_name (str): Hugging Face model identifier for Sentence-BERT.

    Returns:
    - model: Pre-trained Sentence-BERT model.
    """
    try:
        model = SentenceTransformer(model_name)
        return model
    except Exception as e:
        logger.error(f"Failed to load Sentence-BERT model '{model_name}': {e}")
        raise

def generate_issue_description_embedding(description, sentencebert_model, logger):
    """
    Generate an embedding for the given issue description using Sentence-BERT.

    Parameters:
    - description (str): The issue description text.
    - sentencebert_model: Pre-trained Sentence-BERT model.
    - logger: Logger instance for logging.

    Returns:
    - List of floats representing the embedding vector.
    """
    if not description.strip():
        logger.warning("Empty issue description provided for embedding.")
        return None

    try:
        # Generate embedding
        embedding = sentencebert_model.encode(description, show_progress_bar=False)
        return embedding.tolist()
    except Exception as e:
        logger.error(f"Error during issue description embedding generation: {e}")
        return None



# ----- End embeddings.py -----

# ----- Begin gitcodes.py -----
import os, sys, git  # type: ignore
import requests # type: ignore

def get_repo(path, logger):
    """
    Get the Git repository at the specified path.

    Parameters:
    - path (str): Path to the cloned Git repository.
    - logger: Logger instance for logging.

    Returns:
    - repo: Git repository object.
    """
    logger.info(f"Attempting to get repository at path: {path}")
    try:
        repo = git.Repo(path)
        logger.info(f"Successfully obtained repository: {repo.working_tree_dir}")
        return repo
    except git.exc.InvalidGitRepositoryError:
        logger.error(f"Invalid Git repository at path: {path}")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Unexpected error accessing repository: {e}")
        sys.exit(1)

def get_repo(path, logger):
    """
    Get the Git repository at the specified path.

    Parameters:
    - path (str): Path to the cloned Git repository.
    - logger: Logger instance for logging.

    Returns:
    - repo: Git repository object.
    """
    logger.info(f"Attempting to get repository at path: {path}")
    try:
        repo = git.Repo(path)
        logger.info(f"Successfully obtained repository: {repo.working_tree_dir}")
        return repo
    except git.exc.InvalidGitRepositoryError:
        logger.error(f"Invalid Git repository at path: {path}")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Unexpected error accessing repository: {e}")
        sys.exit(1)

def get_issue_commits(repo, issue_number, logger):
    """
    Retrieve commits related to a specific issue.

    Parameters:
    - repo: Git repository object.
    - issue_number (int): Issue number to search for.
    - logger: Logger instance for logging.

    Returns:
    - issue_commits (list): List of commit objects related to the issue.
    """
    logger.info(f"Searching for commits related to issue number: {issue_number}")
    issue_commits = []
    try:
        for commit in repo.iter_commits():
            if f'#{issue_number}' in commit.message:
                logger.debug(f"Found commit related to issue #{issue_number}: {commit.hexsha}")
                issue_commits.append(commit)
        logger.info(f"Total commits found for issue #{issue_number}: {len(issue_commits)}")
        return issue_commits
    except Exception as e:
        logger.error(f"Error while fetching commits for issue #{issue_number}: {e}")
        return issue_commits

def get_commit_files(repo, commit_hash, logger):
    """
    Get files changed in a specific commit.

    Parameters:
    - repo: Git repository object.
    - commit_hash (str): SHA hash of the commit.
    - logger: Logger instance for logging.

    Returns:
    - files (dict): Dictionary of files changed in the commit with stats.
    """
    logger.info(f"Getting files changed in commit: {commit_hash}")
    try:
        commit_to_check = repo.commit(commit_hash)
        files = commit_to_check.stats.files
        logger.debug(f"Files changed in commit {commit_hash}: {files}")
        return files
    except Exception as e:
        logger.error(f"Error fetching files for commit {commit_hash}: {e}")
        return {}

def get_issue_description(issue_number, owner, repo_name, logger):
    """
    Fetch the description of a GitHub issue.

    Parameters:
    - issue_number (int): Issue number.
    - owner (str): Repository owner.
    - repo_name (str): Repository name.
    - logger: Logger instance for logging.

    Returns:
    - description (str): Description of the issue.
    """
    token = os.environ.get('GITHUB_TOKEN')
    url = f"https://api.github.com/repos/{owner}/{repo_name}/issues/{issue_number}"
    headers = {"Authorization": f"token {token}"} if token else {}
    logger.info(f"Fetching description for issue #{issue_number}")
    try:
        response = requests.get(url, headers=headers)
        if response.status_code == 200:
            description = response.json().get('body', f"Issue #{issue_number}")
            logger.debug(f"Issue #{issue_number} description fetched successfully.")
            return description
        else:
            logger.warning(f"Issue #{issue_number} not found or access denied. Status Code: {response.status_code}")
            return f"Issue #{issue_number} (not found)"
    except Exception as e:
        logger.error(f"Error fetching issue description for #{issue_number}: {e}")
        return f"Issue #{issue_number} (error fetching description)"

def get_codebase_before_commit(repo, commit, logger):
    """
    Extract the codebase right before the specified commit.

    Parameters:
    - repo: Git repository object.
    - commit: Git commit object.
    - logger: Logger instance for logging.

    Returns:
    - codebase (str): Concatenated contents of all relevant files before the commit.
    """
    logger.info(f"Extracting codebase before commit: {commit.hexsha}")
    try:
        parents = commit.parents
        if not parents:
            logger.warning(f"Commit {commit.hexsha} has no parents. Returning empty codebase.")
            return ""
        parent_commit = parents[0]
        codebase = ""
        tree = parent_commit.tree
        total_len = 0
        for blob in tree.traverse():
            if blob.type == 'blob':
                try:
                    # Filter for specific file types if necessary
                    if not blob.path.endswith(('.py', '.js', '.java', '.cpp', '.c', '.rb', '.go', '.ts')):
                        continue

                    # Decode the blob content as UTF-8 text
                    content = blob.data_stream.read().decode('utf-8')
                    total_len += len(content)
                    codebase += content + "\n"
                except UnicodeDecodeError:
                    # Skip binary files or files with decoding issues
                    logger.debug(f"Skipping non-text file: {blob.path}")
                    continue
        # print('='*10)
        # print(total_len)
        # print('='*10)

        logger.info(f"Extracted codebase before commit {commit.hexsha}")
        return codebase
    except Exception as e:
        logger.error(f"Error extracting codebase before commit {commit.hexsha}: {e}")
        return ""

# ----- End gitcodes.py -----

# ----- Begin main.py -----
import argparse
import sys

from _logging import setup_logging
from dataset import create_issue_dataset
from embeddings import load_codebert_model, load_sentencebert_model
from gitcodes import get_repo
from _globals import OWNER, REPO_NAME


def main():
    """
    Main function to create the issue dataset.

    Parses command-line arguments, sets up logging, loads the models, accesses the repository,
    and initiates dataset creation based on the provided issue range.
    """
    # Initialize argument parser
    parser = argparse.ArgumentParser(
        description='Create a dataset of GitHub issues with related commit information and codebase embeddings.',
        epilog='Example usage: python IssueDatasetCreator.py /path/to/repo 1 100',
        formatter_class=argparse.RawTextHelpFormatter
    )

    # Define positional arguments
    parser.add_argument(
        'repo_path',
        type=str,
        help='Path to the cloned GitHub repository.'
    )
    parser.add_argument(
        'start_issue_number',
        type=int,
        help='Starting issue number (inclusive).'
    )
    parser.add_argument(
        'end_issue_number',
        type=int,
        help='Ending issue number (inclusive).'
    )

    # Define optional arguments
    parser.add_argument(
        '-o', '--output',
        type=str,
        default='data/issue_dataset.jsonl',
        help='Path to the output JSON Lines (.jsonl) file. Default: data/issue_dataset.jsonl'
    )
    parser.add_argument(
        '-l', '--log',
        type=str,
        default='dataset_creation.log',
        help='Path to the log file. Default: dataset_creation.log'
    )
    parser.add_argument(
        '--owner',
        type=str,
        default=OWNER,
        help=f"Repository owner. Default: '{OWNER}'"
    )
    parser.add_argument(
        '--repo',
        type=str,
        default=REPO_NAME,
        help=f"Repository name. Default: '{REPO_NAME}'"
    )
    parser.add_argument(
        '--description-embedder',
        type=str,
        default='sentence-transformers/all-MiniLM-L6-v2',
        help="Sentence-BERT model for embedding issue descriptions. Default: 'sentence-transformers/all-MiniLM-L6-v2'"
    )

    # Parse arguments
    args = parser.parse_args()

    # Setup logging
    logger = setup_logging(log_file=args.log)

    # Log script start
    logger.info("Starting Issue Dataset Creation Script.")

    # Validate issue range
    if args.start_issue_number > args.end_issue_number:
        logger.error("Start issue number must be less than or equal to end issue number.")
        sys.exit(1)

    # Load CodeBERT model and tokenizer
    logger.info("Loading CodeBERT model and tokenizer...")
    try:
        tokenizer, model = load_codebert_model()
        logger.info("CodeBERT model and tokenizer loaded successfully.")
    except Exception as e:
        logger.error(f"Error loading CodeBERT model: {e}")
        sys.exit(1)

    # Load Sentence-BERT model
    logger.info("Loading Sentence-BERT model for issue description embeddings...")
    try:
        sentencebert_model = load_sentencebert_model(model_name=args.description_embedder, logger=logger)
        logger.info("Sentence-BERT model loaded successfully.")
    except Exception as e:
        logger.error(f"Error loading Sentence-BERT model: {e}")
        sys.exit(1)

    # Get Git repository
    repo = get_repo(args.repo_path, logger)

    # Define issue numbers
    issue_numbers = range(args.start_issue_number, args.end_issue_number + 1)

    # Create dataset
    create_issue_dataset(
        repo=repo,
        issue_numbers=issue_numbers,
        tokenizer=tokenizer,
        model=model,
        sentencebert_model=sentencebert_model,
        owner=args.owner,
        repo_name=args.repo,
        logger=logger,
        output_file=args.output
    )

    logger.info("Issue Dataset Creation Script finished.")


if __name__ == "__main__":
    main()

# ----- End main.py -----

# ----- Begin metrics.py -----


# ----- End metrics.py -----

